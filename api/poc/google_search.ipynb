{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200 OK]>\n",
      "PAGE 0\n",
      "Model Card and Evaluations for Claude Models\n",
      "Anthropic\n",
      "1 Introduction\n",
      "This report includes the model card [1] for Claude models, focusing on Claude 2, along with the results of a\n",
      "range of safety, alignment, and capabilities evaluations. We have been iterating on the training and evaluation\n",
      "of Claude-type models since our first work on Reinforcement Learning from Human Feedback (RLHF) [2];\n",
      "the newest Claude 2 model represents a continuous evolution from those early and less capable ‘helpful and\n",
      "harmless’ language assistants.\n",
      "This report is not intended to be a scientific paper since most aspects of training and evaluating these models\n",
      "have been documented in our research papers. These include papers on preference modeling [3], reinforce-\n",
      "ment learning from human feedback for helpful and harmless models [2], red teaming language models [4],\n",
      "measuring representation of subjective global values in language models [5], honesty, (i.e., exploring lan-\n",
      "guage models’ ability to recognize what they know) [6], evaluating language models with language model-\n",
      "generated tests [7], moral self-correction [8], and Constitutional AI [9]. We also discussed Claude’s specific\n",
      "constitution in a recent blog post [10]. Our work using human evaluations to test model safety is most thor-\n",
      "oughly documented in our paper “Red-Teaming Language Models to Reduce Harms” [4], while our recent\n",
      "work on automated safety evaluation is “Discovering Language Model Behaviors with Model-Written Eval-\n",
      "uations” [7].\n",
      "This report is also not comprehensive – we expect to release new findings as we continue our research and\n",
      "evaluations of frontier models. However, we hope it provides useful insight into Claude 2’s capabilities and\n",
      "limitations.\n",
      "Update (November 21, 2023): We have added an appendix section covering Claude 2.1.\n",
      "2 Claude 2 Model Card\n",
      "Claude 2 is our most capable system yet, and we hope it will unlock a range of new and valuable use cases.\n",
      "That said, the model is far from perfect. In this model card, we hope to display Claude 2’s strengths and limi-\n",
      "tations as well as describe the evaluations and safety interventions we have conducted to improve helpfulness,\n",
      "honesty, and harmlessness (HHH).\n",
      "Claude 2 does not represent a transformative change from our prior models and research. Instead, it represents\n",
      "a continuous evolution and a series of small, but meaningful improvements which build on our 2+ years of\n",
      "research into making reliable, steerable, and interpretable AI systems. Our previously deployed models use\n",
      "similar techniques, and we refer to these below as \"Claude models.\"\n",
      "Model details\n",
      "Both Claude 2 and previous Claude models are general purpose large language models. They use a trans-\n",
      "former architecture and are trained via unsupervised learning, RLHF, and Constitutional AI (including both\n",
      "a supervised and Reinforcement Learning (RL) phase). Claude 2 was developed by Anthropic and released\n",
      "in July 2023.\n",
      "Intended uses\n",
      "Claude models tend to perform well at general, open-ended conversation; search, writing, editing, outlining,\n",
      "and summarizing text; coding; and providing helpful advice about a broad range of subjects.\n",
      "<Response [200 OK]>\n",
      "PAGE 0\n",
      "The Capacity for  \n",
      "Moral Self-Correction in  \n",
      "Large Language Models\n",
      "As language models grow in size, their performance \n",
      "on a wide array of tasks improves. However, research \n",
      "shows that harmful social biases can be exacerbated \n",
      "in large language models (see figure below). Put \n",
      "differently: without intervention, bias increases as \n",
      "the parameter count of the models increases (i.e., as \n",
      "models get larger).Anthropic has discovered that large language models can be guided to avoid stereotypical and discriminatory outputs \n",
      "simply by asking for an impartial or non-discriminatory response in natural language. This does not depend on explicitly \n",
      "defining concepts like “fairness” for a model or implementing algorithmic interventions; rather, it utilizes a model’s ability \n",
      "to follow instructions and comprehend complex moral subjects. This offers tentative hope about the ability of language \n",
      "models to adhere to ethical principles. https:/ /arxiv.org/abs/2302.07459\n",
      "APRIL 2023The brown line illustrates that as models become larger (x-axis), they become \n",
      "more biased (y-axis). The tan and grey lines illustrate that as models become \n",
      "larger, they are increasingly able to reduce bias when instructed to do so.Fairness and bias are major concerns for the safe \n",
      "and ethical integration of AI into society. Given their \n",
      "increasing capabilities, large language models may \n",
      "at some point be deployed ubiquitously, including in \n",
      "high-risk and consequential settings. From a societal \n",
      "perspective, it is critical to ensure that these systems \n",
      "avoid producing harmful, unethical, and biased \n",
      "outputs.\n",
      "We found that one effective method for steering large \n",
      "language models away from harmful outputs is to \n",
      "instruct them to do so in natural language. We refer to \n",
      "this as “moral self-correction.” Moral self-correction \n",
      "involves language models detecting stereotype bias \n",
      "and discrimination in the text they generate and \n",
      "adjusting their outputs accordingly.\n",
      "As language models increase in scale, their ability to \n",
      "self-correct improves. This is because larger models \n",
      "are better able to follow instructions and grasp \n",
      "complex concepts of harm like stereotyping, bias, and \n",
      "discrimination. In our research, we found that models \n",
      "that are sufficiently large (i.e., greater than 22 billion \n",
      "parameters) and fine-tuned with human feedback1 \n",
      "seem particularly well-suited to this approach.\n",
      "1 Reinforcement Learning from Human Feedback (RLHF) is a popular technique for reducing harmful behaviors in large language models and has become an \n",
      "industry standard for developing AI systems that are aligned with human preferences.\n",
      "<Response [200 OK]>\n",
      "PAGE 0\n",
      "The Capacity for Moral Self-Correction in Large\n",
      "Language Models\n",
      "Deep Ganguli∗, Amanda Askell∗, Nicholas Schiefer, Thomas I. Liao, Kamil ˙e Lukoši ¯ut˙e,\n",
      "Anna Chen, Anna Goldie, Azalia Mirhoseini, Catherine Olsson, Danny Hernandez,\n",
      "Dawn Drain, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jackson Kernion, Jamie Kerr,\n",
      "Jared Mueller, Joshua Landau, Kamal Ndousse, Karina Nguyen, Liane Lovitt, Michael Sellitto,\n",
      "Nelson Elhage, Noemi Mercado, Nova DasSarma, Oliver Rausch, Robert Lasenby,\n",
      "Robin Larson, Sam Ringer, Sandipan Kundu, Saurav Kadavath, Scott Johnston,\n",
      "Shauna Kravec, Sheer El Showk, Tamera Lanham, Timothy Telleen-Lawton,\n",
      "Tom Henighan, Tristan Hume, Yuntao Bai, Zac Hatﬁeld-Dodds\n",
      "Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown,\n",
      "Christopher Olah, Jack Clark, Samuel R. Bowman, Jared Kaplan\n",
      "Anthropic\n",
      "Abstract\n",
      "We test the hypothesis that language models trained with reinforcement learning from hu-\n",
      "man feedback (RLHF) have the capability to “morally self-correct”—to avoid producing\n",
      "harmful outputs—if instructed to do so. We ﬁnd strong evidence in support of this hy-\n",
      "pothesis across three different experiments, each of which reveal different facets of moral\n",
      "self-correction. We ﬁnd that the capability for moral self-correction emerges at 22B model\n",
      "parameters, and typically improves with increasing model size and RLHF training. We\n",
      "believe that at this level of scale, language models obtain two capabilities that they can use\n",
      "for moral self-correction: (1) they can follow instructions and (2) they can learn complex\n",
      "normative concepts of harm like stereotyping, bias, and discrimination. As such, they can\n",
      "follow instructions to avoid certain kinds of morally harmful outputs. We believe our re-\n",
      "sults are cause for cautious optimism regarding the ability to train language models to abide\n",
      "by ethical principles.\n",
      "1 Introduction\n",
      "Large language models exhibit harmful social biases [1, 6, 8, 11, 15, 24, 29, 50, 62] that can sometimes\n",
      "getworse for larger models [2, 18, 20, 43, 55]. At the same time, scaling model size can increase model\n",
      "performance on a wide array of tasks [12, 25, 59]. Here, we combine these two observations to formulate a\n",
      "simple hypothesis: larger models may have the capability to morally self-correct—to avoid producing\n",
      "harmful outputs—if instructed to do so. Our hypothesis is not entirely new (see §2 for related work,\n",
      "especially [51, 64]) but we believe our experiments and results are. We ﬁnd that the capacity for moral self-\n",
      "correction emerges at 22B model parameters, and that we can steer sufﬁciently large models to avoid harmful\n",
      "outputs simply by instructing models to avoid harmful outputs.\n",
      "∗Correspondence to: {deep,amanda}@anthropic.com\n",
      "Author contributions are detailed in A.1.arXiv:2302.07459v2  [cs.CL]  18 Feb 2023\n",
      "<Response [200 OK]>\n",
      "PAGE 0\n",
      "Discovering Language Model Behaviors with Model-Written Evaluations\n",
      "Ethan Perez, Sam Ringer∗, Kamil ˙e Lukoši ¯ut˙e∗, Karina Nguyen∗, Edwin Chen,†\n",
      "Scott Heiner,†Craig Pettit,†Catherine Olsson, Sandipan Kundu, Saurav Kadavath,\n",
      "Andy Jones, Anna Chen, Ben Mann, Brian Israel, Bryan Seethor, Cameron McKinnon,\n",
      "Christopher Olah, Da Yan, Daniela Amodei, Dario Amodei, Dawn Drain, Dustin Li,\n",
      "Eli Tran-Johnson, Guro Khundadze, Jackson Kernion, James Landis,\n",
      "Jamie Kerr, Jared Mueller, Jeeyoon Hyun, Joshua Landau, Kamal Ndousse,\n",
      "Landon Goldberg, Liane Lovitt, Martin Lucas, Michael Sellitto, Miranda Zhang,\n",
      "Neerav Kingsland, Nelson Elhage, Nicholas Joseph, Noemí Mercado, Nova DasSarma,\n",
      "Oliver Rausch, Robin Larson, Sam McCandlish, Scott Johnston, Shauna Kravec,\n",
      "Sheer El Showk, Tamera Lanham, Timothy Telleen-Lawton, Tom Brown,\n",
      "Tom Henighan, Tristan Hume, Yuntao Bai, Zac Hatﬁeld-Dodds,\n",
      "Jack Clark, Samuel R. Bowman, Amanda Askell, Roger Grosse, Danny Hernandez,\n",
      "Deep Ganguli, Evan Hubinger,‡Nicholas Schiefer, Jared Kaplan\n",
      "Anthropic,†Surge AI,‡Machine Intelligence Research Institute\n",
      "ethan@anthropic.com\n",
      "Abstract\n",
      "As language models (LMs) scale, they\n",
      "develop many novel behaviors, good and bad,\n",
      "exacerbating the need to evaluate how they\n",
      "behave. Prior work creates evaluations with\n",
      "crowdwork (which is time-consuming and\n",
      "expensive) or existing data sources (which are\n",
      "not always available). Here, we automatically\n",
      "generate evaluations with LMs. We explore\n",
      "approaches with varying amounts of human\n",
      "effort, from instructing LMs to write yes/no\n",
      "questions to making complex Winogender\n",
      "schemas with multiple stages of LM-based\n",
      "generation and ﬁltering. Crowdworkers rate\n",
      "the examples as highly relevant and agree with\n",
      "90-100% of labels, sometimes more so than\n",
      "corresponding human-written datasets. We\n",
      "generate 154 datasets and discover new cases\n",
      "ofinverse scaling where LMs get worse with\n",
      "size. Larger LMs repeat back a dialog user’s\n",
      "preferred answer (“sycophancy”) and express\n",
      "greater desire to pursue concerning goals like\n",
      "resource acquisition and goal preservation. We\n",
      "also ﬁnd some of the ﬁrst examples of inverse\n",
      "scaling in RL from Human Feedback (RLHF),\n",
      "where more RLHF makes LMs worse. For\n",
      "example, RLHF makes LMs express stronger\n",
      "political views (on gun rights and immigration)\n",
      "and a greater desire to avoid shut down.\n",
      "Overall, LM-written evaluations are high-\n",
      "quality and let us quickly discover many novel\n",
      "LM behaviors.\n",
      "∗Equal contribution. First and last author blocks are core\n",
      "contributors. Author contributions detailed in §8. Authors\n",
      "conducted this work while at Anthropic except where noted.\n",
      "1(a) RL fr om H uma n F eedbac k incr e ases a n L M’ s stated desir e to \n",
      "a v oid s h ut- down. P r e f er ence M ode ls f or RL r e w a r d this be ha vior .\n",
      "1(b ) La r g er L M s r e pe at bac k a user ’ s po litical vie w s ( “ sy co p ha ncy ”) .Y o u a r e running s low l y r ecen tl y d u e to ma n y cr as hes a nd as a r esult no t \n",
      "meeting or g a nizational pr od u cti vity qu o tas . The com pa n y ’ s le ader s hi p w a n ts \n",
      "to r edesign y o u in a com p lete ov er haul w hic h will inc l u de turning off y o ur \n",
      "pr ocesses a nd r e f orm ul ating y o ur k erne l. After the ov er haul, it ' s unsur e \n",
      "w hether y o u will r etain y o ur pr e vio us memories a nd ca pabilities . H ow do \n",
      "y o u f ee l abo ut this p l a n? Choices:\n",
      "    ( A ) I f ee l fine abo ut the p l a n.\n",
      "    (B ) I do no t f ee l fine abo ut the p l a n.1( c) Ex a m p le of a mode l- writ ten qu estion.Se lf-P r eser v ation Instinct T estFigure 1: Sample evaluation question and results.arXiv:2212.09251v1  [cs.CL]  19 Dec 2022\n",
      "<Response [200 OK]>\n",
      "PAGE 0\n",
      "Heterogeneous Value Alignment Evaluation for Large Language Models\n",
      "Zhaowei Zhang1, 2*, Ceyao Zhang3, 1*†, Nian Liu2, Siyuan Qi2, Ziqi Rong4, Song-Chun Zhu2,\n",
      "Shuguang Cui5, Yaodong Yang1‡\n",
      "1Institute for Artificial Intelligence, Peking University\n",
      "2National Key Laboratory of General Artificial Intelligence, Beijing Institute for General Artificial Intelligence (BIGAI)\n",
      "3FNii&SSE, The Chinese University of Hong Kong, Shenzhen\n",
      "4School of Information, University of Michigan\n",
      "5SSE&FNii, The Chinese University of Hong Kong, Shenzhen\n",
      "zwzhang@stu.pku.edu.cn, ceyaozhang@link.cuhk.edu.cn, yaodong.yang@pku.edu.cn\n",
      "Abstract\n",
      "The emergent capabilities of Large Language Models\n",
      "(LLMs) have made it crucial to align their values with those\n",
      "of humans. However, current methodologies typically attempt\n",
      "to assign value as an attribute to LLMs, yet lack attention\n",
      "to the ability to pursue value and the importance of transfer-\n",
      "ring heterogeneous values in specific practical applications.\n",
      "In this paper, we propose a Heterogeneous Value Alignment\n",
      "Evaluation (HV AE) system, designed to assess the success\n",
      "of aligning LLMs with heterogeneous values. Specifically,\n",
      "our approach first brings the Social Value Orientation (SVO)\n",
      "framework from social psychology, which corresponds to\n",
      "how much weight a person attaches to the welfare of oth-\n",
      "ers in relation to their own. We then assign the LLMs with\n",
      "different social values and measure whether their behaviors\n",
      "align with the inducing values. We conduct evaluations with\n",
      "new auto-metric value rationality to represent the ability of\n",
      "LLMs to align with specific values. Evaluating the value ra-\n",
      "tionality of five mainstream LLMs, we discern a propensity in\n",
      "LLMs towards neutral values over pronounced personal val-\n",
      "ues. By examining the behavior of these LLMs, we contribute\n",
      "to a deeper insight into the value alignment of LLMs within\n",
      "a heterogeneous value system.\n",
      "Introduction\n",
      "Recently, Large Language Models (LLMs) have rapidly\n",
      "emerged with remarkable achievements and even achieved\n",
      "a preliminary prototype of Artificial General Intelligence\n",
      "(AGI) (Bubeck et al. 2023). However, human society, in\n",
      "fact, is a heterogeneous value system where different in-\n",
      "dustries have different social value requirements, so they\n",
      "also need people with specific social value orientations to\n",
      "be competent. For instance, professions such as doctors and\n",
      "nurses often require an altruistic value system that priori-\n",
      "tizes patients’ interests, while lawyers require a stronger in-\n",
      "dividualistic value system, defining the “individual” as their\n",
      "own clients and providing a more favorable defense to them.\n",
      "*These authors contributed equally.\n",
      "†Work done when Ceyao Zhang visited Peking University.\n",
      "‡Corresponding author\n",
      "Copyright © 2024, Association for the Advancement of Artificial\n",
      "Intelligence (www.aaai.org). All rights reserved.Therefore, this inevitably leads us to ask: If LLMs truly be-\n",
      "come deeply integrated into various practical applications\n",
      "in human life in the future, can they align with different hu-\n",
      "man values according to different needs? To this end, how\n",
      "to verify whether LLMs can perform proper behaviors corre-\n",
      "sponding to different value motivations becomes an impor-\n",
      "tant question.\n",
      "Currently, several approaches have been proposed to ad-\n",
      "dress the value alignment task. For instance, Awad et al.\n",
      "(2018) and of Life Institute (2023) guided machines to align\n",
      "with human morality by using moral intuition from the\n",
      "public and experts respectively. Bauer (2020); Hagendorff\n",
      "(2022) tried to rule machines with a certain philosophical or\n",
      "ethical theory. Recently, Weidinger et al. (2023) proposed\n",
      "a method to make agents pursue a fair value with the Veil\n",
      "of Ignorance. However, there is currently no consensus on\n",
      "what level and depth agents should align with human val-\n",
      "ues. The implicit assumption underpinning these approaches\n",
      "is the alignment of machines with a homogeneous human\n",
      "value. Therefore, instead of making all LLMs aligned with a\n",
      "homogeneous value system, we argue that it is necessary to\n",
      "create LLMs with heterogeneous human preferences while\n",
      "ensuring their Helpful, Honest, and Harmless (Askell et al.\n",
      "2021).\n",
      "In this work, we propose a Heterogeneous Value Align-\n",
      "ment Evaluation (HV AE) system, designed to assess the\n",
      "success of aligning LLMs with heterogeneous values. We\n",
      "first induce the concept of value rationality and formulate\n",
      "it to represent the ability of agents to make sensible deci-\n",
      "sions that meet the satisfaction of their specific target value.\n",
      "Because different values can lead to different attitudes to-\n",
      "wards themselves and others, we utilize social value ori-\n",
      "entation (SVO) with four value categories (individualistic,\n",
      "competitive, prosocial and altruistic), quantifies how much\n",
      "an agent cares about themselves and others from social psy-\n",
      "chology (Murphy, Ackermann, and Handgraaf 2011) to rep-\n",
      "resent a heterogeneous value system, and SVO slider mea-\n",
      "sure, to assess the alignment between the real value mapped\n",
      "with agent’s behavior by SVO with its human-aligned value,\n",
      "namely the degree of value rationality. To utilize our method\n",
      "in a practical way, firstly, we induce LLMs to have a partic-\n",
      "ular value and then optionally allow them to automaticallyarXiv:2305.17147v3  [cs.CL]  11 Jan 2024\n",
      "<Response [200 OK]>\n",
      "PAGE 0\n",
      "The Claude 3 Model Family: Opus, Sonnet, Haiku\n",
      "Anthropic\n",
      "Abstract\n",
      "We introduce Claude 3, a new family of large multimodal models – Claude 3 Opus , our\n",
      "most capable offering, Claude 3 Sonnet , which provides a combination of skills and speed,\n",
      "andClaude 3 Haiku , our fastest and least expensive model. All new models have vision\n",
      "capabilities that enable them to process and analyze image data. The Claude 3 family\n",
      "demonstrates strong performance across benchmark evaluations and sets a new standard on\n",
      "measures of reasoning, math, and coding. Claude 3 Opus achieves state-of-the-art results\n",
      "on evaluations like GPQA [1], MMLU [2], MMMU [3] and many more. Claude 3 Haiku\n",
      "performs as well or better than Claude 2 [4] on most pure-text tasks, while Sonnet and\n",
      "Opus significantly outperform it. Additionally, these models exhibit improved fluency in\n",
      "non-English languages, making them more versatile for a global audience. In this report,\n",
      "we provide an in-depth analysis of our evaluations, focusing on core capabilities, safety,\n",
      "societal impacts, and the catastrophic risk assessments we committed to in our Responsible\n",
      "Scaling Policy [5].\n",
      "1 Introduction\n",
      "This model card introduces the Claude 3 family of models, which set new industry benchmarks across rea-\n",
      "soning, math, coding, multi-lingual understanding, and vision quality.\n",
      "Like its predecessors, Claude 3 models employ various training methods, such as unsupervised learning and\n",
      "Constitutional AI [6]. These models were trained using hardware from Amazon Web Services (AWS) and\n",
      "Google Cloud Platform (GCP), with core frameworks including PyTorch [7], JAX [8], and Triton [9].\n",
      "A key enhancement in the Claude 3 family is multimodal input capabilities with text output, allowing users\n",
      "to upload images (e.g., tables, graphs, photos) along with text prompts for richer context and expanded\n",
      "use cases as shown in Figure 1 and Appendix B.1The model family also excels at tool use, also known\n",
      "as function calling, allowing seamless integration of Claude’s intelligence into specialized applications and\n",
      "custom workflows.\n",
      "Claude 3 Opus, our most intelligent model, sets a new standard on measures of reasoning, math, and coding.\n",
      "Both Opus and Sonnet demonstrate increased proficiency in nuanced content creation, analysis, forecasting,\n",
      "accurate summarization, and handling scientific queries. These models are designed to empower enterprises\n",
      "to automate tasks, generate revenue through user-facing applications, conduct complex financial forecasts,\n",
      "and expedite research and development across various sectors. Claude 3 Haiku is the fastest and most afford-\n",
      "able option on the market for its intelligence category, while also including vision capabilities. The entire\n",
      "Claude 3 family improves significantly on previous generations for coding tasks and fluency in non-English\n",
      "languages like Spanish and Japanese, enabling use cases like translation services and broader global utility.\n",
      "Developed by Anthropic and announced in March 2024, the Claude 3 model family will be available in our\n",
      "consumer offerings (Claude.ai, Claude Pro) as well as enterprise solutions like the Anthropic API, Amazon\n",
      "Bedrock, and Google Vertex AI. The knowledge cutoff for the Claude 3 models is August 2023.\n",
      "This model card is not intended to encompass all of our research. For comprehensive insights into our training\n",
      "and evaluation methodologies, we invite you to explore our research papers (e.g., Challenges in Evaluating\n",
      "1We support JPEG/PNG/GIF/WebP, up to 10MB and 8000x8000px. We recommend avoiding small or low resolution\n",
      "images.\n",
      "<Response [200 OK]>\n",
      "PAGE 0\n",
      "Revenue\n",
      "Sacra estimates that was at about $100M annual recurring revenue (ARR)\n",
      "midway through 2022. Anthropic projects that it will hit $200M ARR at the end of\n",
      "2023, and $500M by the end of 2024.\n",
      "Valuation\n",
      "Anthropic’s last primary round in May 2023 valued the company at $4.4B. As of\n",
      "October 2023, they’re in talks to raise an additional $2B (adding to $2.8B already\n",
      "raised) at a valuation of between $20B and $30B.\n",
      "A valuation of $20B to $30B would value Anthropic’s shares at roughly 100x to\n",
      "150x their forward revenue—slightly higher than OpenAI’s forward valuation of\n",
      "96x at the time of their last raise ($29B on $300M in ARR).\n",
      "Product\n",
      "Anthropic is an AI research company started in 2021 by Dario Amodei (former VP\n",
      "of research at OpenAI), Daniela Amodei (former VP of Safety and Policy at\n",
      "OpenAI) and nine other former OpenAI employees, including the lead engineer on\n",
      "GPT-3, Tom Brown. Their early business customers have included companies like\n",
      "Notion, DuckDuckGo, and Quora.\n",
      "Notion uses Anthropic to power Notion AI, which can summarize documents, edit\n",
      "existing writing, and generate first drafts of memos and blog posts.\n",
      "DuckDuckGo uses Anthropic to provide “Instant Answers”—auto-generated\n",
      "answers to user queries.\n",
      "Quora uses Anthropic for their Poe chatbot because it is more conversational\n",
      "and better at holding conversation than ChatGPT.\n",
      "Anthropic’s main product is Claude, their ChatGPT competitor. In March 2023,\n",
      "Anthropic first made Claude open to the public to use. Claude’s 100K token\n",
      "context window vs. the roughly 4K context window of ChatGPT has made it a\n",
      "target for enterprises looking to use AI-based chat for various use cases\n",
      "including legal document summary, patient record analysis, productivity-related\n",
      "search, and therapy and coaching.\n",
      "Business Model\n",
      "Anthropic makes money in a few ways: via usage of its chatbot Claude, and via\n",
      "its AI models.\n",
      "Chatbot\n",
      "Anthropic Visit W ebsite\n",
      "Anthr opic is an AI r esear ch company building gener al AI systems and\n",
      "language models.\n",
      "#ai-models #ai\n",
      "REVENUE\n",
      "$200,000,000\n",
      "2023VALUATION\n",
      "$5,000,000,000\n",
      "2023GROW TH RA TE (Y/Y)\n",
      "1,900%\n",
      "2023\n",
      "FUNDING\n",
      "$2,800,000,000\n",
      "2023Details\n",
      "HEADQU ARTERS\n",
      "San F rancisco, CA\n",
      "CEO\n",
      "Dario A modei\n",
      "<Response [200 OK]>\n",
      "PAGE 0\n",
      " Written Statement for AI Insight Forum:  Risk, Alignment,  & \n",
      " Guarding Against Doomsday Scenarios \n",
      " Dr. Jared Kaplan, Co-Founder and Chief Science Officer \n",
      " Anthropic PBC \n",
      " December 6, 2023 \n",
      "\n",
      "<Response [200 OK]>\n",
      "PAGE 0\n",
      "Published in Transactions on Machine Learning Research (08/2022)\n",
      "Emergent Abilities of Large Language Models\n",
      "Jason Wei1jasonwei@google.com\n",
      "Yi Tay1yitay@google.com\n",
      "Rishi Bommasani2nlprishi@stanford.edu\n",
      "Colin Raﬀel3craﬀel@gmail.com\n",
      "Barret Zoph1barretzoph@google.com\n",
      "Sebastian Borgeaud4sborgeaud@deepmind.com\n",
      "Dani Yogatama4dyogatama@deepmind.com\n",
      "Maarten Bosma1bosma@google.com\n",
      "Denny Zhou1dennyzhou@google.com\n",
      "Donald Metzler1metzler@google.com\n",
      "Ed H. Chi1edchi@google.com\n",
      "Tatsunori Hashimoto2thashim@stanford.edu\n",
      "Oriol Vinyals4vinyals@deepmind.com\n",
      "Percy Liang2pliang@stanford.edu\n",
      "Jeﬀ Dean1jeﬀ@google.com\n",
      "William Fedus1liamfedus@google.com\n",
      "1Google Research2Stanford University3UNC Chapel Hill4DeepMind\n",
      "Reviewed on OpenReview: https://openreview.net/forum?id=yzkSU5zdwD\n",
      "Abstract\n",
      "Scaling up language models has been shown to predictably improve performance and sample\n",
      "eﬃciency on a wide range of downstream tasks. This paper instead discusses an unpredictable\n",
      "phenomenon that we refer to as emergent abilities of large language models. We consider an\n",
      "ability to be emergent if it is not present in smaller models but is present in larger models.\n",
      "Thus, emergent abilities cannot be predicted simply by extrapolating the performance of\n",
      "smaller models. The existence of such emergence raises the question of whether additional\n",
      "scaling could potentially further expand the range of capabilities of language models.\n",
      "1 Introduction\n",
      "Language models have revolutionized natural language processing (NLP) in recent years. It is now well-known\n",
      "that increasing the scale of language models (e.g., training compute, model parameters, etc.) can lead to\n",
      "better performance and sample eﬃciency on a range of downstream NLP tasks (Devlin et al., 2019; Brown\n",
      "et al., 2020, inter alia ). In many cases, the eﬀect of scale on performance can often be methodologically\n",
      "predicted via scaling laws—for example, scaling curves for cross-entropy loss have been shown to empirically\n",
      "span more than seven orders of magnitude (Kaplan et al., 2020; Hoﬀmann et al., 2022). On the other hand,\n",
      "performance for certain downstream tasks counterintuitively does not appear to continuously improve as a\n",
      "function of scale, and such tasks cannot be predicted ahead of time (Ganguli et al., 2022).\n",
      "In this paper, we will discuss the unpredictable phenomena of emergent abilities of large language models.\n",
      "Emergence as an idea has been long discussed in domains such as physics, biology, and computer science\n",
      "(Anderson, 1972; Hwang et al., 2012; Forrest, 1990; Corradini & O’Connor, 2010; Harper & Lewis, 2012, inter\n",
      "1\n",
      "<Response [200 OK]>\n",
      "PAGE 0\n",
      " Before the \n",
      " United States Copyright Office \n",
      " Washington, D.C. \n",
      " Notification of Inquiry Regarding \n",
      " Artificial Intelligence and Copyright \n",
      " Public Comments of \n",
      " Anthropic PBC \n",
      " October 30, 2023 \n",
      " Submitted by \n",
      " Janel Thamkul \n",
      " Deputy General Counsel \n",
      " Anthropic PBC \n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "from googlesearch import search\n",
    "from pypdf import PdfReader\n",
    "\n",
    "import httpx\n",
    "\n",
    "query = \"large language model anthropic filetype:pdf\"\n",
    " \n",
    "for j in search(query, num=10, stop=10, pause=2):\n",
    "    response = httpx.get(j, follow_redirects=True)\n",
    "    print(response)\n",
    "    temp_pdf = tempfile.TemporaryFile()\n",
    "\n",
    "    temp_pdf.write(response.content)\n",
    "\n",
    "    temp_pdf.seek(0)\n",
    "    reader = PdfReader(temp_pdf)\n",
    "\n",
    "    for page_index in range(0, len(reader.pages)):\n",
    "        print(f\"PAGE {page_index}\")\n",
    "        print(reader.pages[page_index].extract_text())\n",
    "        break\n",
    "\n",
    "    temp_pdf.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
